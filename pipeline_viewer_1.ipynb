{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ffe91dc6-cd11-4d0c-93e4-5ecbc25a3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import networkx as nx\n",
    "import time\n",
    "import heapq\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from sys import platform\n",
    "import time\n",
    "!chmod +x spike-dasm.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "99bb1677-3365-4b67-be85-06b78e537a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON\n",
    "with open('rv64um-v-mul.out', 'r') as f:\n",
    "    json_lines = f.readlines()\n",
    "\n",
    "inst_jsons = []\n",
    "for line in json_lines:\n",
    "    try:\n",
    "        inst_jsons.append(json.loads(line))\n",
    "    except json.JSONDecodeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ce2027d-c05a-4e95-a192-5b6b81a8c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_names = [\"IF1\", \"IF2\", \"IBuf\", \"EX\", \"MEM\", \"DIV\", \"WB\", \"LLWB\", \"COM\"]\n",
    "start_stage = \"IF1\"\n",
    "split_points = [\"IF2\"]\n",
    "end_stages = [\"COM\", \"LLWB\"]\n",
    "event_types = [\"bytes\", \"pc\", \"bytes\", \"inst_bytes\", \"bytes\", \"bytes\", \"bytes\", \"bytes\", \"byte\"]\n",
    "event_to_datatype = {e:d for e, d in zip(event_names, event_types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2175508c-e9bc-49b8-8cce-50636d3d63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_array(jsons):\n",
    "    dasm_input = \"\"\n",
    "    for json in jsons:\n",
    "        if event_to_datatype[json[\"event_name\"]] == \"inst_bytes\":\n",
    "            dasm_input += \"DASM(\" + json[\"data\"] + \")|\"\n",
    "        else:\n",
    "            dasm_input += json[\"data\"] + \"|\"\n",
    "    dasm_input = dasm_input[:-1]\n",
    "    if platform == \"darwin\":\n",
    "        p = Popen(\"./spike-dasm --isa=rv64gcv\", stdout=PIPE, stdin=PIPE, stderr=PIPE, text=True, shell=True)\n",
    "    else:\n",
    "        p = Popen(\"./spike-dasm.exe --isa=rv64gcv\", stdout=PIPE, stdin=PIPE, stderr=PIPE, text=True, shell=True)\n",
    "    stdout_data = p.communicate(input=dasm_input)[0]\n",
    "    insts = stdout_data.split(\"|\")\n",
    "    return np.array(insts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "23883804-1914-4498-b134-d096230e3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_ids = np.array([inst_jsons[i][\"id\"] for i in range(len(inst_jsons))])\n",
    "inst_cycle = np.array([inst_jsons[i][\"cycle\"].strip() for i in range(len(inst_jsons))])\n",
    "inst_event = np.array([inst_jsons[i][\"event_name\"] for i in range(len(inst_jsons))])\n",
    "data_field = generate_data_array(inst_jsons)\n",
    "inst_parent = np.array([inst_jsons[i][\"parents\"] for i in range(len(inst_jsons))])\n",
    "data = np.column_stack((inst_ids,inst_parent, inst_cycle, inst_event, data_field))\n",
    "columns = [\"inst_id\", \"parent_id\", \"cycle\", \"stage\", \"data\"]\n",
    "df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5ab1b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(df):\n",
    "    DG = nx.DiGraph()\n",
    "    for row in df.itertuples():\n",
    "        DG.add_node(row.inst_id, cycle=row.cycle, data=row.data, stage=row.stage)\n",
    "        if row.parent_id != \"None\":\n",
    "            DG.add_edge(row.parent_id, row.inst_id)\n",
    "    return DG            \n",
    "\n",
    "def construct_speculative_trace(G):\n",
    "    paths = []\n",
    "    id = 0\n",
    "    for node in G:\n",
    "        data = G.nodes[node]\n",
    "        if G.in_degree(node) == 0: # root node\n",
    "            new_paths = trace_down(G, node, [], [])\n",
    "            paths.extend(new_paths)\n",
    "    for path in paths:\n",
    "        if path[-1][0] not in end_stages:\n",
    "            path.append((\"FLUSH\", str(int(path[-1][1]) + 1), None))\n",
    "        else:\n",
    "            path.append((\"KONNATA_RET\", str(int(path[-1][1]) + 1), None))\n",
    "        path.insert(0, (id, path[0][-1]))\n",
    "    return paths\n",
    "\n",
    "def trace_down(G, node, curr_path, paths):\n",
    "    data = G.nodes[node]\n",
    "    curr_path.append((data[\"stage\"], data[\"cycle\"], data[\"data\"]))\n",
    "    if G.out_degree(node) == 0: # terminal node\n",
    "        paths.append(curr_path)\n",
    "        return paths\n",
    "    succs = list(DG.successors(node))\n",
    "    if data[\"stage\"] not in split_points and len(succs) > 1:\n",
    "        inst_paths = [trace_down(G, n, [], [])[0] for n in succs]\n",
    "        inst_paths = inst_paths[1] + inst_paths[0]\n",
    "        inst_paths.sort(key=lambda x: x[1])\n",
    "        paths.append(curr_path + inst_paths)\n",
    "    else:\n",
    "        for n in succs:\n",
    "            paths.extend(trace_down(G, n, curr_path[:], []))\n",
    "    return paths\n",
    "\n",
    "def construct_committed_trace(G):\n",
    "    paths = []\n",
    "    id = 0\n",
    "    for node in G:\n",
    "        data = G.nodes[node]\n",
    "        if G.out_degree(node) == 0 and data[\"stage\"] in end_stages: # committed leaf node\n",
    "            new_path = trace_up(G, node)\n",
    "            new_path.insert(0, (id, data[\"data\"]))\n",
    "            paths.append(new_path)\n",
    "            id += 1\n",
    "    return paths\n",
    "\n",
    "def trace_up(G, node):\n",
    "    path = []\n",
    "    while node:\n",
    "        data = G.nodes[node]\n",
    "        path.insert(0, (data[\"stage\"], data[\"cycle\"], data[\"data\"]))\n",
    "        node = list(DG.predecessors(node))[0] if list(DG.predecessors(node)) else \"\"\n",
    "    return path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "432470dc-7863-4d7b-9217-d14bd1bd89db",
   "metadata": {},
   "outputs": [],
   "source": [
    "DG = construct_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f3e35642-e5c5-484b-a436-46d1ffe4e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = construct_committed_trace(DG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3547b799-9016-4190-95bc-8e16edfe1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_paths = construct_speculative_trace(DG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a0bde9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def convert_to_kanata(threads, verbose=False):\n",
    "    pq = []\n",
    "    id = 0\n",
    "    if not verbose:\n",
    "        threads = list(filter(lambda x: x[-1][0] == 'KONNATA_RET', threads)) #Relies on the last element of inst list being RET\n",
    "    for inst in threads:\n",
    "        for stage in inst[1:]:\n",
    "            heapq.heappush(pq, ((int(stage[1])), (id, stage[2], stage[0]))) #Min heap of (cycle -> (unique_id, pc, pipeline stage))\n",
    "        id += 1\n",
    "            \n",
    "    with open('mulf.log', 'w') as file:\n",
    "        file.write('Kanata    0004\\n')\n",
    "        cycle, (id, pc, stage) = heapq.heappop(pq)\n",
    "        prev_cycle = cycle\n",
    "        file.write(f'C=\\t{cycle}\\n')\n",
    "        while pq:\n",
    "            cycle_diff = cycle - prev_cycle\n",
    "            if (cycle_diff > 0):\n",
    "                file.write(f\"C\\t{cycle_diff}\\n\")\n",
    "            if (stage == start_stage):\n",
    "                file.write(f\"I\\t{id}\\t{cycle}\\t0\\n\")\n",
    "                # file.write(f\"L    {id}    0    {pc}\\n\")\n",
    "            if (stage == 'KONNATA_RET'):\n",
    "                file.write(f\"R\\t{id}\\t{id}\\t0\\n\")\n",
    "            elif (stage == 'FLUSH'):\n",
    "                file.write(f\"R\\t{id}\\t{id}\\t1\\n\")\n",
    "            elif (event_to_datatype[stage] == \"inst_bytes\"):\n",
    "                file.write(f\"S\\t{id}\\t0\\t{stage}\\n\")\n",
    "                file.write(f\"L\\t{id}\\t0\\tLabel:{pc}\\n\")\n",
    "            elif (event_to_datatype[stage] == \"pc\"):\n",
    "                file.write(f\"S\\t{id}\\t0\\t{stage}\\n\")\n",
    "                file.write(f\"L\\t{id}\\t0\\tPC:{pc} \\n\")\n",
    "            else:\n",
    "                file.write(f\"S\\t{id}\\t0\\t{stage}\\n\")\n",
    "                file.write(f\"L\\t{id}\\t1\\tPC:{pc}\\n\")\n",
    "\n",
    "            prev_cycle = cycle\n",
    "            cycle, (id, pc, stage) = heapq.heappop(pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ffe44ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_kanata(spec_paths, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac62457-2c9e-4631-a7a3-4102d8f80c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
